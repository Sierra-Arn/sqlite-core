{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81d8e55d",
   "metadata": {},
   "source": [
    "# **Setup Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af90efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change directory to the project root so that relative paths in .env \n",
    "# (e.g., SQLite db path) resolve correctly\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d42d20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('.')\n",
    "\n",
    "from app.db.async_db import MLModelService, MLMetricService\n",
    "from app.db.models import DeviceType\n",
    "from app.db.schemas import MLModelRequestCreate, MLModelRequestUpdate, MLMetricRequestCreate, MLMetricRequestUpdate\n",
    "\n",
    "ml_model_service = MLModelService()\n",
    "ml_metric_service = MLMetricService()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15d1c89",
   "metadata": {},
   "source": [
    "# **Test**\n",
    "\n",
    "## **I. Test `ml_model_service`**\n",
    "\n",
    "### **i. Test `create()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3150b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_create_ml_model(name: str, device: DeviceType):\n",
    "    try:\n",
    "        model_data = MLModelRequestCreate(name=name, device=device)\n",
    "        created_model = await ml_model_service.create(model_data)\n",
    "        print(f\"ML model successfully created:\")\n",
    "        print(created_model.model_dump_json(indent=2))\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Business logic error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "await test_create_ml_model(name=\"test_model_v1\", device=DeviceType.CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e522863f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to create an ML model with a name that already exists\n",
    "await test_create_ml_model(name=\"test_model_v1\", device=DeviceType.CUDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69001e9",
   "metadata": {},
   "source": [
    "### **ii. Test `get()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dfde71",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_get_ml_model_by_id(model_id: int):\n",
    "    try:\n",
    "        model = await ml_model_service.get(model_id)\n",
    "        \n",
    "        if model:\n",
    "            print(f\"ML model found:\")\n",
    "            print(model.model_dump_json(indent=2))\n",
    "        else:\n",
    "            print(f\"ML model with ID {model_id} not found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving model: {e}\")\n",
    "\n",
    "await test_get_ml_model_by_id(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c7a34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to get an ML model with a non-existent ID\n",
    "await test_get_ml_model_by_id(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ca4550",
   "metadata": {},
   "source": [
    "### **iii. Test `get_all()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62ca78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a helper record to ensure get_all returns all entries, not just one\n",
    "await test_create_ml_model(name=\"cool_model\", device=DeviceType.CUDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc3a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_get_all_ml_models(skip: int = 0, limit: int = 10):\n",
    "    try:\n",
    "        models = await ml_model_service.get_all(skip=skip, limit=limit)\n",
    "        print(f\"Found {len(models)} ML models:\")\n",
    "        for model in models:\n",
    "            print(model.model_dump_json(indent=2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving models: {e}\")\n",
    "\n",
    "await test_get_all_ml_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9831f3f9",
   "metadata": {},
   "source": [
    "### **iv. Test `update()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6189a8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_update_ml_model(model_id: int, device: DeviceType):\n",
    "    try:\n",
    "        update_data = MLModelRequestUpdate(device=device)\n",
    "        updated_model = await ml_model_service.update(model_id, update_data)\n",
    "\n",
    "        if updated_model:\n",
    "            print(f\"ML model successfully updated:\")\n",
    "            print(updated_model.model_dump_json(indent=2))\n",
    "        else:\n",
    "            print(f\"ML model with ID {model_id} not found\")\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"Business logic error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        \n",
    "await test_update_ml_model(model_id=1, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372848c",
   "metadata": {},
   "outputs": [],
   "source": [
    "await test_update_ml_model(model_id=1, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a87692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to update an ML model with a non-existent ID\n",
    "await test_update_ml_model(model_id=99, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442532fe",
   "metadata": {},
   "source": [
    "### **v. Test `delete()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79aa62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_delete_ml_model(model_id: int):\n",
    "    try:\n",
    "        result = await ml_model_service.delete(model_id)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"ML model with ID {model_id} successfully deleted\")\n",
    "        else:\n",
    "            print(f\"ML model with ID {model_id} not found\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting model: {e}\")\n",
    "\n",
    "await test_delete_ml_model(model_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3f6189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Trying to delete an ML model with a non-existent ID\n",
    "await test_delete_ml_model(model_id=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28967c7a",
   "metadata": {},
   "source": [
    "## **II. Test `ml_metric_service`**\n",
    "\n",
    "### **i. Test `create()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcecf977",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_create_ml_metric(name: str, value: float, ml_model_id: int):\n",
    "    try:\n",
    "        metric_data = MLMetricRequestCreate(name=name, value=value, ml_model_id=ml_model_id)\n",
    "        created_metric = await ml_metric_service.create(metric_data)\n",
    "        print(f\"ML metric successfully created:\")\n",
    "        print(created_metric.model_dump_json(indent=2))\n",
    "        \n",
    "    except ValueError as e:\n",
    "        print(f\"Business logic error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "\n",
    "# Assuming model with ID 2 exists (from previous test_create_ml_model)\n",
    "await test_create_ml_metric(name=\"accuracy\", value=0.95, ml_model_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d9bbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to create a metric for a non-existent model\n",
    "await test_create_ml_metric(name=\"f1_score\", value=0.88, ml_model_id=999)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bdd207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to create a duplicate metric for the same model\n",
    "await test_create_ml_metric(name=\"accuracy\", value=0.96, ml_model_id=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ab0eb",
   "metadata": {},
   "source": [
    "### **ii. Test `get()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d48c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_get_ml_metric_by_id(metric_id: int):\n",
    "    try:\n",
    "        metric = await ml_metric_service.get(metric_id)\n",
    "        \n",
    "        if metric:\n",
    "            print(f\"ML metric found:\")\n",
    "            print(metric.model_dump_json(indent=2))\n",
    "        else:\n",
    "            print(f\"ML metric with ID {metric_id} not found\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving metric: {e}\")\n",
    "\n",
    "await test_get_ml_metric_by_id(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd17400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to get a metric with a non-existent ID\n",
    "await test_get_ml_metric_by_id(999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "effab79f",
   "metadata": {},
   "source": [
    "### **iii. Test `get_all()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a5de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another metric to ensure get_all returns multiple entries\n",
    "await test_create_ml_metric(name=\"latency_ms\", value=45.2, ml_model_id=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af79dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_get_all_ml_metrics(skip: int = 0, limit: int = 10):\n",
    "    try:\n",
    "        metrics = await ml_metric_service.get_all(skip=skip, limit=limit)\n",
    "        print(f\"Found {len(metrics)} ML metrics:\")\n",
    "        for metric in metrics:\n",
    "            print(metric.model_dump_json(indent=2))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error retrieving metrics: {e}\")\n",
    "\n",
    "await test_get_all_ml_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba4e59",
   "metadata": {},
   "source": [
    "### **iv. Test `update()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae435658",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_update_ml_metric(metric_id: int, value: float):\n",
    "    try:\n",
    "        update_data = MLMetricRequestUpdate(value=value)\n",
    "        updated_metric = await ml_metric_service.update(metric_id, update_data)\n",
    "\n",
    "        if updated_metric:\n",
    "            print(f\"ML metric successfully updated:\")\n",
    "            print(updated_metric.model_dump_json(indent=2))\n",
    "        else:\n",
    "            print(f\"ML metric with ID {metric_id} not found\")\n",
    "    \n",
    "    except ValueError as e:\n",
    "        print(f\"Business logic error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error: {e}\")\n",
    "        \n",
    "await test_update_ml_metric(metric_id=1, value=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47929a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to update a metric with a non-existent ID\n",
    "await test_update_ml_metric(metric_id=999, value=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367d5b38",
   "metadata": {},
   "source": [
    "### **v. Test `delete()` method**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49c11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_delete_ml_metric(metric_id: int):\n",
    "    try:\n",
    "        result = await ml_metric_service.delete(metric_id)\n",
    "        \n",
    "        if result:\n",
    "            print(f\"ML metric with ID {metric_id} successfully deleted\")\n",
    "        else:\n",
    "            print(f\"ML metric with ID {metric_id} not found\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error deleting metric: {e}\")\n",
    "\n",
    "await test_delete_ml_metric(metric_id=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9fdbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to delete a metric with a non-existent ID\n",
    "await test_delete_ml_metric(metric_id=999)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
